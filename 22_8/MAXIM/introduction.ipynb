{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献\n",
    "+ https://zhuanlan.zhihu.com/p/481256924\n",
    "+ code:https://github.com/vztu/maxim-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优势\n",
    "首先，MAXIM在任意大的图像上具有全局接受野，时间复杂度为线性。\n",
    "\n",
    "其次，它直接支持任意输入分辨率。\n",
    "\n",
    "最后，它提供了局部（Conv）和全局（MLP）块的平衡设计，表现优于SOTA方法，而无需进行大规模预训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLPBlock\n",
    "MLPBlock+CALayer=RDCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_initializer = nn.initializers.normal(stddev=2e-2)\n",
    "class MlpBlock(nn.Module):\n",
    "  \"\"\"A 1-hidden-layer MLP block, applied over the last dimension.\"\"\"\n",
    "  mlp_dim: int\n",
    "  dropout_rate: float = 0.0\n",
    "  use_bias: bool = True\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, deterministic=True):\n",
    "    n, h, w, d = x.shape\n",
    "    x = nn.Dense(self.mlp_dim, use_bias=self.use_bias,\n",
    "                 kernel_init=weight_initializer)(x)\n",
    "    x = nn.gelu(x)\n",
    "    x = nn.Dropout(rate=self.dropout_rate)(x, deterministic)\n",
    "    x = nn.Dense(d, use_bias=self.use_bias,\n",
    "                 kernel_init=weight_initializer)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CALayer\n",
    "RCAB、RDCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CALayer(nn.Module):\n",
    "  \"\"\"Squeeze-and-excitation block for channel attention.\n",
    "\n",
    "  ref: https://arxiv.org/abs/1709.01507\n",
    "  \"\"\"\n",
    "  features: int\n",
    "  reduction: int = 4\n",
    "  use_bias: bool = True\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    # 2D global average pooling\n",
    "    y = jnp.mean(x, axis=[1, 2], keepdims=True)\n",
    "    # Squeeze (in Squeeze-Excitation)\n",
    "    y = Conv1x1(self.features // self.reduction, use_bias=self.use_bias)(y)\n",
    "    y = nn.relu(y)\n",
    "    # Excitation (in Squeeze-Excitation)\n",
    "    y = Conv1x1(self.features, use_bias=self.use_bias)(y)\n",
    "    y = nn.sigmoid(y)\n",
    "    return x * y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RCAB\n",
    "Encoder\n",
    "\n",
    "残差卷积通道注意力块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCAB(nn.Module):\n",
    "  \"\"\"Residual channel attention block. Contains LN,Conv,lRelu,Conv,SELayer.\"\"\"\n",
    "  features: int\n",
    "  reduction: int = 4\n",
    "  lrelu_slope: float = 0.2\n",
    "  use_bias: bool = True\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    shortcut = x\n",
    "    x = nn.LayerNorm(name=\"LayerNorm\")(x)\n",
    "    x = Conv3x3(features=self.features, use_bias=self.use_bias, name=\"conv1\")(x)\n",
    "    x = nn.leaky_relu(x, negative_slope=self.lrelu_slope)\n",
    "    x = Conv3x3(features=self.features, use_bias=self.use_bias, name=\"conv2\")(x)\n",
    "    \n",
    "    x = CALayer(features=self.features, reduction=self.reduction,\n",
    "                use_bias=self.use_bias, name=\"channel_attention\")(x)\n",
    "    return x + shortcut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDCAB\n",
    "bottleneckBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RDCAB(nn.Module):\n",
    "  \"\"\"Residual dense channel attention block. Used in Bottlenecks.\"\"\"\n",
    "  features: int\n",
    "  reduction: int = 16\n",
    "  use_bias: bool = True\n",
    "  dropout_rate: float = 0.0\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, deterministic=True):\n",
    "    y = nn.LayerNorm(name=\"LayerNorm\")(x)\n",
    "    y = MlpBlock(\n",
    "        mlp_dim=self.features,\n",
    "        dropout_rate=self.dropout_rate,\n",
    "        use_bias=self.use_bias,\n",
    "        name=\"channel_mixing\")(\n",
    "            y, deterministic=deterministic)\n",
    "    y = CALayer(\n",
    "        features=self.features,\n",
    "        reduction=self.reduction,\n",
    "        use_bias=self.use_bias,\n",
    "        name=\"channel_attention\")(\n",
    "            y)\n",
    "    x = x + y\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GetSpatialGatingWeights\n",
    "CGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_images_einops(x, patch_size):\n",
    "  \"\"\"Image to patches.\"\"\"\n",
    "  batch, height, width, channels = x.shape\n",
    "  grid_height = height // patch_size[0]\n",
    "  grid_width = width // patch_size[1]\n",
    "  x = einops.rearrange(\n",
    "      x, \"n (gh fh) (gw fw) c -> n (gh gw) (fh fw) c\",\n",
    "      gh=grid_height, gw=grid_width, fh=patch_size[0], fw=patch_size[1])\n",
    "  return x\n",
    "\n",
    "\n",
    "def unblock_images_einops(x, grid_size, patch_size):\n",
    "  \"\"\"patches to images.\"\"\"\n",
    "  x = einops.rearrange(\n",
    "      x, \"n (gh gw) (fh fw) c -> n (gh fh) (gw fw) c\",\n",
    "      gh=grid_size[0], gw=grid_size[1], fh=patch_size[0], fw=patch_size[1])\n",
    "  return x\n",
    "\n",
    "\n",
    "\n",
    "class GetSpatialGatingWeights(nn.Module):\n",
    "  \"\"\"Get gating weights for cross-gating MLP block.\"\"\"\n",
    "  features: int\n",
    "  block_size: Sequence[int]\n",
    "  grid_size: Sequence[int]\n",
    "  input_proj_factor: int = 2\n",
    "  dropout_rate: float = 0.0\n",
    "  use_bias: bool = True\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, deterministic):\n",
    "    n, h, w, num_channels = x.shape\n",
    "\n",
    "    # input projection\n",
    "    x = nn.LayerNorm(name=\"LayerNorm_in\")(x)\n",
    "    x = nn.Dense(\n",
    "        num_channels * self.input_proj_factor,\n",
    "        use_bias=self.use_bias,\n",
    "        name=\"in_project\")(\n",
    "            x)\n",
    "    x = nn.gelu(x)\n",
    "    u, v = jnp.split(x, 2, axis=-1)\n",
    "\n",
    "    # Get grid MLP weights\n",
    "    gh, gw = self.grid_size\n",
    "    fh, fw = h // gh, w // gw\n",
    "    u = block_images_einops(u, patch_size=(fh, fw))\n",
    "    dim_u = u.shape[-3]\n",
    "    u = jnp.swapaxes(u, -1, -3)\n",
    "    u = nn.Dense(\n",
    "        dim_u, use_bias=self.use_bias, kernel_init=nn.initializers.normal(2e-2),\n",
    "        bias_init=nn.initializers.ones)(u)\n",
    "    u = jnp.swapaxes(u, -1, -3)\n",
    "    u = unblock_images_einops(u, grid_size=(gh, gw), patch_size=(fh, fw))\n",
    "\n",
    "    # Get Block MLP weights\n",
    "    fh, fw = self.block_size\n",
    "    gh, gw = h // fh, w // fw\n",
    "    v = block_images_einops(v, patch_size=(fh, fw))\n",
    "    dim_v = v.shape[-2]\n",
    "    v = jnp.swapaxes(v, -1, -2)\n",
    "    v = nn.Dense(\n",
    "        dim_v, use_bias=self.use_bias, kernel_init=nn.initializers.normal(2e-2),\n",
    "        bias_init=nn.initializers.ones)(v)\n",
    "    v = jnp.swapaxes(v, -1, -2)\n",
    "    v = unblock_images_einops(v, grid_size=(gh, gw), patch_size=(fh, fw))\n",
    "\n",
    "    x = jnp.concatenate([u, v], axis=-1)\n",
    "    x = nn.Dense(num_channels, use_bias=self.use_bias, name=\"out_project\")(x)\n",
    "    x = nn.Dropout(self.dropout_rate)(x, deterministic)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResidualSplitHeadMultiAxisGmlpLayer\n",
    "bottleneckBlock、Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center>\n",
    "<img src=.\\img\\Multi-axis_gated_MLP.png width=\"80%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上图所示，输入的特征首先进行通道映射，然后分成两个头，分别进行全局和局部交互。其中一半的头进入局部分支（图中红色），我们使用 gMLP 算子在**固定的窗口大小内**进行局部空间交互；另一半头喂进全局分支（图中绿色），我们同样使用 gMLP 算子在**固定的网格**位置进行全局（膨胀）空间交互。值得注意的是，图中的 **Block 和 Grid 操作均为窗口划分（和Swin一样），但Block操作中我们固定【窗口大小】，而在 Grid 操作中我们固定【窗口数量】（or 网格大小）**。在两个并行分支结构中，我们每次只对固定维度的坐标进行操作，而在其他坐标都共享参数，从而实现了同时具有“全卷积”属性和全局/局部感受野。由于我们总是使用固定的窗口大小和网格大小，该模块也具有线性计算复杂度  。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridGatingUnit(nn.Module):\n",
    "  \"\"\"A SpatialGatingUnit as defined in the gMLP paper.\n",
    "  The 'spatial' dim is defined as the second last.\n",
    "  If applied on other dims, you should swapaxes first.\n",
    "  \"\"\"\n",
    "  use_bias: bool = True\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    u, v = jnp.split(x, 2, axis=-1)\n",
    "    v = nn.LayerNorm(name=\"intermediate_layernorm\")(v)\n",
    "    n = x.shape[-3]   # get spatial dim\n",
    "    v = jnp.swapaxes(v, -1, -3)\n",
    "    v = nn.Dense(n, use_bias=self.use_bias, kernel_init=weight_initializer)(v)\n",
    "    v = jnp.swapaxes(v, -1, -3)\n",
    "    return u * (v + 1.)\n",
    "\n",
    "\n",
    "class GridGmlpLayer(nn.Module):\n",
    "  \"\"\"Grid gMLP layer that performs global mixing of tokens.\"\"\"\n",
    "  grid_size: Sequence[int]\n",
    "  use_bias: bool = True\n",
    "  factor: int = 2\n",
    "  dropout_rate: float = 0.0\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, deterministic=True):\n",
    "    n, h, w, num_channels = x.shape\n",
    "    gh, gw = self.grid_size\n",
    "    fh, fw = h // gh, w // gw\n",
    "    x = block_images_einops(x, patch_size=(fh, fw))\n",
    "    # gMLP1: Global (grid) mixing part, provides global grid communication.\n",
    "    y = nn.LayerNorm(name=\"LayerNorm\")(x)\n",
    "    y = nn.Dense(num_channels * self.factor, use_bias=self.use_bias,\n",
    "                 kernel_init=weight_initializer, name=\"in_project\")(y)\n",
    "    y = nn.gelu(y)\n",
    "    y = GridGatingUnit(use_bias=self.use_bias, name=\"GridGatingUnit\")(y)\n",
    "    y = nn.Dense(num_channels, use_bias=self.use_bias,\n",
    "                 kernel_init=weight_initializer, name=\"out_project\")(y)\n",
    "    y = nn.Dropout(self.dropout_rate)(y, deterministic)\n",
    "    x = x + y\n",
    "    x = unblock_images_einops(x, grid_size=(gh, gw), patch_size=(fh, fw))\n",
    "    return x\n",
    "\n",
    "\n",
    "class BlockGatingUnit(nn.Module):\n",
    "  \"\"\"A SpatialGatingUnit as defined in the gMLP paper.\n",
    "  The 'spatial' dim is defined as the **second last**.\n",
    "  If applied on other dims, you should swapaxes first.\n",
    "  \"\"\"\n",
    "  use_bias: bool = True\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    u, v = jnp.split(x, 2, axis=-1)\n",
    "    v = nn.LayerNorm(name=\"intermediate_layernorm\")(v)\n",
    "    n = x.shape[-2]  # get spatial dim\n",
    "    v = jnp.swapaxes(v, -1, -2)\n",
    "    v = nn.Dense(n, use_bias=self.use_bias, kernel_init=weight_initializer)(v)\n",
    "    v = jnp.swapaxes(v, -1, -2)\n",
    "    return u * (v + 1.)\n",
    "\n",
    "\n",
    "class BlockGmlpLayer(nn.Module):\n",
    "  \"\"\"Block gMLP layer that performs local mixing of tokens.\"\"\"\n",
    "  block_size: Sequence[int]\n",
    "  use_bias: bool = True\n",
    "  factor: int = 2\n",
    "  dropout_rate: float = 0.0\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, deterministic=True):\n",
    "    n, h, w, num_channels = x.shape\n",
    "    fh, fw = self.block_size\n",
    "    gh, gw = h // fh, w // fw\n",
    "    x = block_images_einops(x, patch_size=(fh, fw))\n",
    "    # MLP2: Local (block) mixing part, provides within-block communication.\n",
    "    y = nn.LayerNorm(name=\"LayerNorm\")(x)\n",
    "    y = nn.Dense(num_channels * self.factor, use_bias=self.use_bias,\n",
    "                 kernel_init=weight_initializer, name=\"in_project\")(y)\n",
    "    y = nn.gelu(y)\n",
    "    y = BlockGatingUnit(use_bias=self.use_bias, name=\"BlockGatingUnit\")(y)\n",
    "    y = nn.Dense(num_channels, use_bias=self.use_bias,\n",
    "                 kernel_init=weight_initializer, name=\"out_project\")(y)\n",
    "    y = nn.Dropout(self.dropout_rate)(y, deterministic)\n",
    "    x = x + y\n",
    "    x = unblock_images_einops(x, grid_size=(gh, gw), patch_size=(fh, fw))\n",
    "    return x\n",
    "\n",
    "\"\"\"允许在多尺度长范围空间混合\"\"\"\n",
    "class ResidualSplitHeadMultiAxisGmlpLayer(nn.Module):\n",
    "  \"\"\"The multi-axis gated MLP block.\"\"\"\n",
    "  block_size: Sequence[int]\n",
    "  grid_size: Sequence[int]\n",
    "  block_gmlp_factor: int = 2\n",
    "  grid_gmlp_factor: int = 2\n",
    "  input_proj_factor: int = 2\n",
    "  use_bias: bool = True\n",
    "  dropout_rate: float = 0.0\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, deterministic=True):\n",
    "    shortcut = x\n",
    "    n, h, w, num_channels = x.shape\n",
    "    x = nn.LayerNorm(name=\"LayerNorm_in\")(x)\n",
    "    x = nn.Dense(num_channels * self.input_proj_factor, use_bias=self.use_bias,\n",
    "                 kernel_init=weight_initializer, name=\"in_project\")(x)\n",
    "    x = nn.gelu(x)\n",
    "\n",
    "    u, v = jnp.split(x, 2, axis=-1)\n",
    "    # GridGMLPLayer\n",
    "    u = GridGmlpLayer(\n",
    "        grid_size=self.grid_size,\n",
    "        factor=self.grid_gmlp_factor,\n",
    "        use_bias=self.use_bias,\n",
    "        dropout_rate=self.dropout_rate,\n",
    "        name=\"GridGmlpLayer\")(u, deterministic)\n",
    "\n",
    "    # BlockGMLPLayer\n",
    "    v = BlockGmlpLayer(\n",
    "        block_size=self.block_size,\n",
    "        factor=self.block_gmlp_factor,\n",
    "        use_bias=self.use_bias,\n",
    "        dropout_rate=self.dropout_rate,\n",
    "        name=\"BlockGmlpLayer\")(v, deterministic)\n",
    "\n",
    "    x = jnp.concatenate([u, v], axis=-1)\n",
    "\n",
    "    x = nn.Dense(num_channels, use_bias=self.use_bias,\n",
    "                 kernel_init=weight_initializer, name=\"out_project\")(x)\n",
    "    x = nn.Dropout(self.dropout_rate)(x, deterministic)\n",
    "    x = x + shortcut\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetEncoderBlock(nn.Module):\n",
    "  \"\"\"Encoder block in MAXIM.\"\"\"\n",
    "  features: int\n",
    "  block_size: Sequence[int]\n",
    "  grid_size: Sequence[int]\n",
    "  num_groups: int = 1\n",
    "  lrelu_slope: float = 0.2\n",
    "  block_gmlp_factor: int = 2\n",
    "  grid_gmlp_factor: int = 2\n",
    "  input_proj_factor: int = 2\n",
    "  channels_reduction: int = 4\n",
    "  dropout_rate: float = 0.0\n",
    "  downsample: bool = True\n",
    "  use_global_mlp: bool = True\n",
    "  use_bias: bool = True\n",
    "  use_cross_gating: bool = False\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x: jnp.ndarray, skip: jnp.ndarray = None,\n",
    "               enc: jnp.ndarray = None, dec: jnp.ndarray = None, *,\n",
    "               deterministic: bool = True) -> jnp.ndarray:\n",
    "    if skip is not None:\n",
    "      x = jnp.concatenate([x, skip], axis=-1)\n",
    "\n",
    "    # convolution-in\n",
    "    x = Conv1x1(self.features, use_bias=self.use_bias)(x)\n",
    "    shortcut_long = x\n",
    "\n",
    "    for i in range(self.num_groups):\n",
    "      if self.use_global_mlp:\n",
    "        x = ResidualSplitHeadMultiAxisGmlpLayer(\n",
    "            grid_size=self.grid_size,\n",
    "            block_size=self.block_size,\n",
    "            grid_gmlp_factor=self.grid_gmlp_factor,\n",
    "            block_gmlp_factor=self.block_gmlp_factor,\n",
    "            input_proj_factor=self.input_proj_factor,\n",
    "            use_bias=self.use_bias,\n",
    "            dropout_rate=self.dropout_rate,\n",
    "            name=f\"SplitHeadMultiAxisGmlpLayer_{i}\")(x, deterministic)\n",
    "      x = RCAB(\n",
    "          features=self.features,\n",
    "          reduction=self.channels_reduction,\n",
    "          use_bias=self.use_bias,\n",
    "          name=f\"channel_attention_block_1{i}\")(x)\n",
    "\n",
    "    x = x + shortcut_long\n",
    "\n",
    "    if enc is not None and dec is not None:\n",
    "      assert self.use_cross_gating\n",
    "      x, _ = CrossGatingBlock(\n",
    "          features=self.features,\n",
    "          block_size=self.block_size,\n",
    "          grid_size=self.grid_size,\n",
    "          dropout_rate=self.dropout_rate,\n",
    "          input_proj_factor=self.input_proj_factor,\n",
    "          upsample_y=False,\n",
    "          use_bias=self.use_bias,\n",
    "          name=\"cross_gating_block\")(\n",
    "              x, enc + dec, deterministic=deterministic)\n",
    "\n",
    "    if self.downsample:\n",
    "      x_down = Conv_down(self.features, use_bias=self.use_bias)(x)\n",
    "      return x_down, x\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetDecoderBlock(nn.Module):\n",
    "  \"\"\"Decoder block in MAXIM.\"\"\"\n",
    "  features: int\n",
    "  block_size: Sequence[int]\n",
    "  grid_size: Sequence[int]\n",
    "  num_groups: int = 1\n",
    "  lrelu_slope: float = 0.2\n",
    "  block_gmlp_factor: int = 2\n",
    "  grid_gmlp_factor: int = 2\n",
    "  input_proj_factor: int = 2\n",
    "  channels_reduction: int = 4\n",
    "  dropout_rate: float = 0.0\n",
    "  downsample: bool = True\n",
    "  use_global_mlp: bool = True\n",
    "  use_bias: bool = True\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x: jnp.ndarray, bridge: jnp.ndarray = None,\n",
    "               deterministic: bool = True) -> jnp.ndarray:\n",
    "    x = ConvT_up(self.features, use_bias=self.use_bias)(x)\n",
    "\n",
    "    x = UNetEncoderBlock(\n",
    "        self.features,\n",
    "        num_groups=self.num_groups,\n",
    "        lrelu_slope=self.lrelu_slope,\n",
    "        block_size=self.block_size,\n",
    "        grid_size=self.grid_size,\n",
    "        block_gmlp_factor=self.block_gmlp_factor,\n",
    "        grid_gmlp_factor=self.grid_gmlp_factor,\n",
    "        channels_reduction=self.channels_reduction,\n",
    "        use_global_mlp=self.use_global_mlp,\n",
    "        dropout_rate=self.dropout_rate,\n",
    "        downsample=False,\n",
    "        use_bias=self.use_bias)(x, skip=bridge, deterministic=deterministic)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BottleneckBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckBlock(nn.Module):\n",
    "  \"\"\"The bottleneck block consisting of multi-axis gMLP block and RDCAB.\"\"\"\n",
    "  features: int\n",
    "  block_size: Sequence[int]\n",
    "  grid_size: Sequence[int]\n",
    "  num_groups: int = 1\n",
    "  block_gmlp_factor: int = 2\n",
    "  grid_gmlp_factor: int = 2\n",
    "  input_proj_factor: int = 2\n",
    "  channels_reduction: int = 4\n",
    "  dropout_rate: float = 0.0\n",
    "  use_bias: bool = True\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, deterministic):\n",
    "    \"\"\"Applies the Mixer block to inputs.\"\"\"\n",
    "    assert x.ndim == 4  # Input has shape [batch, h, w, c]\n",
    "    n, h, w, num_channels = x.shape\n",
    "\n",
    "    # input projection\n",
    "    x = Conv1x1(self.features, use_bias=self.use_bias, name=\"input_proj\")(x)\n",
    "    shortcut_long = x\n",
    "\n",
    "    for i in range(self.num_groups):\n",
    "      x = ResidualSplitHeadMultiAxisGmlpLayer(\n",
    "          grid_size=self.grid_size,\n",
    "          block_size=self.block_size,\n",
    "          grid_gmlp_factor=self.grid_gmlp_factor,\n",
    "          block_gmlp_factor=self.block_gmlp_factor,\n",
    "          input_proj_factor=self.input_proj_factor,\n",
    "          use_bias=self.use_bias,\n",
    "          dropout_rate=self.dropout_rate,\n",
    "          name=f\"SplitHeadMultiAxisGmlpLayer_{i}\")(x, deterministic)\n",
    "      # Channel-mixing part, which provides within-patch communication.\n",
    "      x = RDCAB(\n",
    "          features=self.features,\n",
    "          reduction=self.channels_reduction,\n",
    "          use_bias=self.use_bias,\n",
    "          name=f\"channel_attention_block_1_{i}\")(\n",
    "              x)\n",
    "\n",
    "    # long skip-connect\n",
    "    x = x + shortcut_long\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CGB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center>\n",
    "<img src=.\\img\\CGB.png width=\"40%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提出了【交叉门控模块】，如 Figure 2(c)所示。其设计理念严格遵守多轴门控MLP模块的模范，同样采用多轴全局/局部交互的gMLP模块。唯一的区别是在提取了gMLP算子的空间门权重（gating weights）后，我们采用了交叉相乘的方式来进行信息交互。例如X,Y是两个不同的输入特征，交叉门控的概念可以简单表示为（具体的公式可以参见文章或代码）：\n",
    "$X=X\\odot Gate(Y);Y=Y\\odot Gate(X)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossGatingBlock(nn.Module):\n",
    "  \"\"\"Cross-gating MLP block.\"\"\"\n",
    "  features: int\n",
    "  block_size: Sequence[int]\n",
    "  grid_size: Sequence[int]\n",
    "  dropout_rate: float = 0.0\n",
    "  input_proj_factor: int = 2\n",
    "  upsample_y: bool = True\n",
    "  use_bias: bool = True\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, y, deterministic=True):\n",
    "    # Upscale Y signal, y is the gating signal.\n",
    "    if self.upsample_y:\n",
    "      y = ConvT_up(self.features, use_bias=self.use_bias)(y)\n",
    "\n",
    "    x = Conv1x1(self.features, use_bias=self.use_bias)(x)\n",
    "    n, h, w, num_channels = x.shape\n",
    "    y = Conv1x1(num_channels, use_bias=self.use_bias)(y)\n",
    "\n",
    "    assert y.shape == x.shape\n",
    "    shortcut_x = x\n",
    "    shortcut_y = y\n",
    "\n",
    "    # Get gating weights from X\n",
    "    x = nn.LayerNorm(name=\"LayerNorm_x\")(x)\n",
    "    x = nn.Dense(num_channels, use_bias=self.use_bias, name=\"in_project_x\")(x)\n",
    "    x = nn.gelu(x)\n",
    "    gx = GetSpatialGatingWeights(\n",
    "        features=num_channels,\n",
    "        block_size=self.block_size,\n",
    "        grid_size=self.grid_size,\n",
    "        dropout_rate=self.dropout_rate,\n",
    "        use_bias=self.use_bias,\n",
    "        name=\"SplitHeadMultiAxisGating_x\")(\n",
    "            x, deterministic=deterministic)\n",
    "\n",
    "    # Get gating weights from Y\n",
    "    y = nn.LayerNorm(name=\"LayerNorm_y\")(y)\n",
    "    y = nn.Dense(num_channels, use_bias=self.use_bias, name=\"in_project_y\")(y)\n",
    "    y = nn.gelu(y)\n",
    "    gy = GetSpatialGatingWeights(\n",
    "        features=num_channels,\n",
    "        block_size=self.block_size,\n",
    "        grid_size=self.grid_size,\n",
    "        dropout_rate=self.dropout_rate,\n",
    "        use_bias=self.use_bias,\n",
    "        name=\"SplitHeadMultiAxisGating_y\")(\n",
    "            y, deterministic=deterministic)\n",
    "\n",
    "    # Apply cross gating: X = X * GY, Y = Y * GX\n",
    "    y = y * gx\n",
    "    y = nn.Dense(num_channels, use_bias=self.use_bias, name=\"out_project_y\")(y)\n",
    "    y = nn.Dropout(self.dropout_rate)(y, deterministic=deterministic)\n",
    "    y = y + shortcut_y\n",
    "\n",
    "    x = x * gy  # gating x using y\n",
    "    x = nn.Dense(num_channels, use_bias=self.use_bias, name=\"out_project_x\")(x)\n",
    "    x = nn.Dropout(self.dropout_rate)(x, deterministic=deterministic)\n",
    "    x = x + y + shortcut_x  # get all aggregated signals\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center>\n",
    "<img src=.\\img\\model.png width=\"90%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAXIM(nn.Module):\n",
    "  \"\"\"The MAXIM model function with multi-stage and multi-scale supervision.\n",
    "  For more model details, please check the CVPR paper:\n",
    "  MAXIM: MUlti-Axis MLP for Image Processing (https://arxiv.org/abs/2201.02973)\n",
    "  Attributes:\n",
    "    features: initial hidden dimension for the input resolution.\n",
    "    depth: the number of downsampling depth for the model.\n",
    "    num_stages: how many stages to use. It will also affects the output list.\n",
    "    num_groups: how many blocks each stage contains.\n",
    "    use_bias: whether to use bias in all the conv/mlp layers.\n",
    "    num_supervision_scales: the number of desired supervision scales.\n",
    "    lrelu_slope: the negative slope parameter in leaky_relu layers.\n",
    "    use_global_mlp: whether to use the multi-axis gated MLP block (MAB) in each\n",
    "      layer.\n",
    "    use_cross_gating: whether to use the cross-gating MLP block (CGB) in the\n",
    "      skip connections and multi-stage feature fusion layers.\n",
    "    high_res_stages: how many stages are specificied as high-res stages. The\n",
    "      rest (depth - high_res_stages) are called low_res_stages.\n",
    "    block_size_hr: the block_size parameter for high-res stages.\n",
    "    block_size_lr: the block_size parameter for low-res stages.\n",
    "    grid_size_hr: the grid_size parameter for high-res stages.\n",
    "    grid_size_lr: the grid_size parameter for low-res stages.\n",
    "    num_bottleneck_blocks: how many bottleneck blocks.\n",
    "    block_gmlp_factor: the input projection factor for block_gMLP layers.\n",
    "    grid_gmlp_factor: the input projection factor for grid_gMLP layers.\n",
    "    input_proj_factor: the input projection factor for the MAB block.\n",
    "    channels_reduction: the channel reduction factor for SE layer.\n",
    "    num_outputs: the output channels.\n",
    "    dropout_rate: Dropout rate.\n",
    "  Returns:\n",
    "    The output contains a list of arrays consisting of multi-stage multi-scale\n",
    "    outputs. For example, if num_stages = num_supervision_scales = 3 (the\n",
    "    model used in the paper), the output specs are: outputs =\n",
    "    [[output_stage1_scale1, output_stage1_scale2, output_stage1_scale3],\n",
    "     [output_stage2_scale1, output_stage2_scale2, output_stage2_scale3],\n",
    "     [output_stage3_scale1, output_stage3_scale2, output_stage3_scale3],]\n",
    "    The final output can be retrieved by outputs[-1][-1].\n",
    "  \"\"\"\n",
    "  features: int = 64\n",
    "  depth: int = 3\n",
    "  num_stages: int = 2\n",
    "  num_groups: int = 1\n",
    "  use_bias: bool = True\n",
    "  num_supervision_scales: int = 1\n",
    "  lrelu_slope: float = 0.2\n",
    "  use_global_mlp: bool = True\n",
    "  use_cross_gating: bool = True\n",
    "  high_res_stages: int = 2\n",
    "  block_size_hr: Sequence[int] = (16, 16)\n",
    "  block_size_lr: Sequence[int] = (8, 8)\n",
    "  grid_size_hr: Sequence[int] = (16, 16)\n",
    "  grid_size_lr: Sequence[int] = (8, 8)\n",
    "  num_bottleneck_blocks: int = 1\n",
    "  block_gmlp_factor: int = 2\n",
    "  grid_gmlp_factor: int = 2\n",
    "  input_proj_factor: int = 2\n",
    "  channels_reduction: int = 4\n",
    "  num_outputs: int = 3\n",
    "  dropout_rate: float = 0.0\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x: jnp.ndarray, *, train: bool = False) -> Any:\n",
    "\n",
    "    n, h, w, c = x.shape  # input image shape\n",
    "    shortcuts = []\n",
    "    shortcuts.append(x)\n",
    "    # Get multi-scale input images\n",
    "    for i in range(1, self.num_supervision_scales):\n",
    "      shortcuts.append(jax.image.resize(\n",
    "          x, shape=(n, h // (2**i), w // (2**i), c), method=\"nearest\"))\n",
    "\n",
    "    # store outputs from all stages and all scales\n",
    "    # Eg, [[(64, 64, 3), (128, 128, 3), (256, 256, 3)],   # Stage-1 outputs\n",
    "    #      [(64, 64, 3), (128, 128, 3), (256, 256, 3)],]  # Stage-2 outputs\n",
    "    outputs_all = []\n",
    "    sam_features, encs_prev, decs_prev = [], [], []\n",
    "\n",
    "    for idx_stage in range(self.num_stages):\n",
    "      # Input convolution, get multi-scale input features\n",
    "      x_scales = []\n",
    "      for i in range(self.num_supervision_scales):\n",
    "        x_scale = Conv3x3(\n",
    "            (2**i) * self.features,\n",
    "            use_bias=self.use_bias,\n",
    "            name=f\"stage_{idx_stage}_input_conv_{i}\")(\n",
    "                shortcuts[i])\n",
    "\n",
    "        # If later stages, fuse input features with SAM features from prev stage\n",
    "        if idx_stage > 0:\n",
    "          # use larger blocksize at high-res stages\n",
    "          if self.use_cross_gating:\n",
    "            block_size = self.block_size_hr if i < self.high_res_stages else self.block_size_lr\n",
    "            grid_size = self.grid_size_hr if i < self.high_res_stages else self.block_size_lr\n",
    "            x_scale, _ = CrossGatingBlock(\n",
    "                features=(2**i) * self.features,\n",
    "                block_size=block_size,\n",
    "                grid_size=grid_size,\n",
    "                dropout_rate=self.dropout_rate,\n",
    "                input_proj_factor=self.input_proj_factor,\n",
    "                upsample_y=False,\n",
    "                use_bias=self.use_bias,\n",
    "                name=f\"stage_{idx_stage}_input_fuse_sam_{i}\")(\n",
    "                    x_scale, sam_features.pop(), deterministic=not train)\n",
    "          else:\n",
    "            x_scale = Conv1x1(\n",
    "                (2**i) * self.features,\n",
    "                use_bias=self.use_bias,\n",
    "                name=f\"stage_{idx_stage}_input_catconv_{i}\")(\n",
    "                    jnp.concatenate(\n",
    "                        [x_scale, sam_features.pop()], axis=-1))\n",
    "\n",
    "        x_scales.append(x_scale)\n",
    "\n",
    "      # start encoder blocks\n",
    "      encs = []\n",
    "      x = x_scales[0]  # First full-scale input feature\n",
    "\n",
    "      for i in range(self.depth):  # 0, 1, 2\n",
    "        # use larger blocksize at high-res stages, vice versa.\n",
    "        block_size = self.block_size_hr if i < self.high_res_stages else self.block_size_lr\n",
    "        grid_size = self.grid_size_hr if i < self.high_res_stages else self.block_size_lr\n",
    "        use_cross_gating_layer = True if idx_stage > 0 else False\n",
    "\n",
    "        # Multi-scale input if multi-scale supervision\n",
    "        x_scale = x_scales[i] if i < self.num_supervision_scales else None\n",
    "\n",
    "        # UNet Encoder block\n",
    "        enc_prev = encs_prev.pop() if idx_stage > 0 else None\n",
    "        dec_prev = decs_prev.pop() if idx_stage > 0 else None\n",
    "\n",
    "        x, bridge = UNetEncoderBlock(\n",
    "            features=(2**i) * self.features,\n",
    "            num_groups=self.num_groups,\n",
    "            downsample=True,\n",
    "            lrelu_slope=self.lrelu_slope,\n",
    "            block_size=block_size,\n",
    "            grid_size=grid_size,\n",
    "            block_gmlp_factor=self.block_gmlp_factor,\n",
    "            grid_gmlp_factor=self.grid_gmlp_factor,\n",
    "            input_proj_factor=self.input_proj_factor,\n",
    "            channels_reduction=self.channels_reduction,\n",
    "            use_global_mlp=self.use_global_mlp,\n",
    "            dropout_rate=self.dropout_rate,\n",
    "            use_bias=self.use_bias,\n",
    "            use_cross_gating=use_cross_gating_layer,\n",
    "            name=f\"stage_{idx_stage}_encoder_block_{i}\")(\n",
    "                x,\n",
    "                skip=x_scale,\n",
    "                enc=enc_prev,\n",
    "                dec=dec_prev,\n",
    "                deterministic=not train)\n",
    "\n",
    "        # Cache skip signals\n",
    "        encs.append(bridge)\n",
    "\n",
    "      # Global MLP bottleneck blocks\n",
    "      for i in range(self.num_bottleneck_blocks):\n",
    "        x = BottleneckBlock(\n",
    "            block_size=self.block_size_lr,\n",
    "            grid_size=self.block_size_lr,\n",
    "            features=(2**(self.depth - 1)) * self.features,\n",
    "            num_groups=self.num_groups,\n",
    "            block_gmlp_factor=self.block_gmlp_factor,\n",
    "            grid_gmlp_factor=self.grid_gmlp_factor,\n",
    "            input_proj_factor=self.input_proj_factor,\n",
    "            dropout_rate=self.dropout_rate,\n",
    "            use_bias=self.use_bias,\n",
    "            channels_reduction=self.channels_reduction,\n",
    "            name=f\"stage_{idx_stage}_global_block_{i}\")(\n",
    "                x, deterministic=not train)\n",
    "      # cache global feature for cross-gating\n",
    "      global_feature = x\n",
    "\n",
    "      # start cross gating. Use multi-scale feature fusion\n",
    "      skip_features = []\n",
    "      for i in reversed(range(self.depth)):  # 2, 1, 0\n",
    "        # use larger blocksize at high-res stages\n",
    "        block_size = self.block_size_hr if i < self.high_res_stages else self.block_size_lr\n",
    "        grid_size = self.grid_size_hr if i < self.high_res_stages else self.block_size_lr\n",
    "\n",
    "        # get additional multi-scale signals\n",
    "        signal = jnp.concatenate([\n",
    "            UpSampleRatio(\n",
    "                (2**i) * self.features,\n",
    "                ratio=2**(j - i),\n",
    "                use_bias=self.use_bias)(enc) for j, enc in enumerate(encs)\n",
    "        ],\n",
    "                                 axis=-1)\n",
    "\n",
    "        # Use cross-gating to cross modulate features\n",
    "        if self.use_cross_gating:\n",
    "          skips, global_feature = CrossGatingBlock(\n",
    "              features=(2**i) * self.features,\n",
    "              block_size=block_size,\n",
    "              grid_size=grid_size,\n",
    "              input_proj_factor=self.input_proj_factor,\n",
    "              dropout_rate=self.dropout_rate,\n",
    "              upsample_y=True,\n",
    "              use_bias=self.use_bias,\n",
    "              name=f\"stage_{idx_stage}_cross_gating_block_{i}\")(\n",
    "                  signal, global_feature, deterministic=not train)\n",
    "        else:\n",
    "          skips = Conv1x1(\n",
    "              (2**i) * self.features, use_bias=self.use_bias)(\n",
    "                  signal)\n",
    "          skips = Conv3x3((2**i) * self.features, use_bias=self.use_bias)(skips)\n",
    "\n",
    "        skip_features.append(skips)\n",
    "\n",
    "      # start decoder. Multi-scale feature fusion of cross-gated features\n",
    "      outputs, decs, sam_features = [], [], []\n",
    "      for i in reversed(range(self.depth)):\n",
    "        # use larger blocksize at high-res stages\n",
    "        block_size = self.block_size_hr if i < self.high_res_stages else self.block_size_lr\n",
    "        grid_size = self.grid_size_hr if i < self.high_res_stages else self.block_size_lr\n",
    "\n",
    "        # get multi-scale skip signals from cross-gating block\n",
    "        signal = jnp.concatenate([\n",
    "            UpSampleRatio(\n",
    "                (2**i) * self.features,\n",
    "                ratio=2**(self.depth - j - 1 - i),\n",
    "                use_bias=self.use_bias)(skip)\n",
    "            for j, skip in enumerate(skip_features)\n",
    "        ],\n",
    "                                 axis=-1)\n",
    "\n",
    "        # Decoder block\n",
    "        x = UNetDecoderBlock(\n",
    "            features=(2**i) * self.features,\n",
    "            num_groups=self.num_groups,\n",
    "            lrelu_slope=self.lrelu_slope,\n",
    "            block_size=block_size,\n",
    "            grid_size=grid_size,\n",
    "            block_gmlp_factor=self.block_gmlp_factor,\n",
    "            grid_gmlp_factor=self.grid_gmlp_factor,\n",
    "            input_proj_factor=self.input_proj_factor,\n",
    "            channels_reduction=self.channels_reduction,\n",
    "            use_global_mlp=self.use_global_mlp,\n",
    "            dropout_rate=self.dropout_rate,\n",
    "            use_bias=self.use_bias,\n",
    "            name=f\"stage_{idx_stage}_decoder_block_{i}\")(\n",
    "                x, bridge=signal, deterministic=not train)\n",
    "\n",
    "        # Cache decoder features for later-stage's usage\n",
    "        decs.append(x)\n",
    "\n",
    "        # output conv, if not final stage, use supervised-attention-block.\n",
    "        if i < self.num_supervision_scales:\n",
    "          if idx_stage < self.num_stages - 1:  # not last stage, apply SAM\n",
    "            sam, output = SAM(\n",
    "                (2**i) * self.features,\n",
    "                output_channels=self.num_outputs,\n",
    "                use_bias=self.use_bias,\n",
    "                name=f\"stage_{idx_stage}_supervised_attention_module_{i}\")(\n",
    "                    x, shortcuts[i], train=train)\n",
    "            outputs.append(output)\n",
    "            sam_features.append(sam)\n",
    "          else:  # Last stage, apply output convolutions\n",
    "            output = Conv3x3(self.num_outputs,\n",
    "                             use_bias=self.use_bias,\n",
    "                             name=f\"stage_{idx_stage}_output_conv_{i}\")(x)\n",
    "            output = output + shortcuts[i]\n",
    "            outputs.append(output)\n",
    "      # Cache encoder and decoder features for later-stage's usage\n",
    "      encs_prev = encs[::-1]\n",
    "      decs_prev = decs\n",
    "\n",
    "      # Store outputs\n",
    "      outputs_all.append(outputs)\n",
    "    return outputs_all\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.2 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1be8e1e697be24aa51be46f7515f4d96c6005120fc689094ce96895b044c9b1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
