{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文表明，训练/测试阶段中基于patch/整个图像的特征汇总的统计数据可能分布方式有很大不同，并导致图像修复的性能退化。基于整个图像的统计数据的分布可能与基于patch的统计数据有很大不同。训练和测试中统计分布的变化可能会导致性能下降，并且以前的工作被广泛忽视。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#作者提出来的AvgPool2d\n",
    "train_size=(1,3,256,256)\n",
    "class AvgPool2d(nn.Module):\n",
    "    def __init__(self, kernel_size=None, base_size=None, auto_pad=True, fast_imp=False):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.base_size = base_size\n",
    "        self.auto_pad = auto_pad\n",
    "\n",
    "        # only used for fast implementation\n",
    "        self.fast_imp = fast_imp\n",
    "        self.rs = [5,4,3,2,1]\n",
    "        self.max_r1 = self.rs[0]\n",
    "        self.max_r2 = self.rs[0]\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return 'kernel_size={}, base_size={}, stride={}, fast_imp={}'.format(\n",
    "            self.kernel_size, self.base_size, self.kernel_size, self.fast_imp\n",
    "        )\n",
    "           \n",
    "    def forward(self, x):\n",
    "        if self.kernel_size is None and self.base_size:\n",
    "            if isinstance(self.base_size, int):\n",
    "                self.base_size = (self.base_size, self.base_size)\n",
    "            self.kernel_size = list(self.base_size)\n",
    "            self.kernel_size[0] = x.shape[2]*self.base_size[0]//train_size[-2]\n",
    "            self.kernel_size[1] = x.shape[3]*self.base_size[1]//train_size[-1]\n",
    "            \n",
    "            # only used for fast implementation\n",
    "            self.max_r1 = max(1, self.rs[0]*x.shape[2]//train_size[-2])\n",
    "            self.max_r2 = max(1, self.rs[0]*x.shape[3]//train_size[-1])\n",
    "\n",
    "        if self.fast_imp:   # Non-equivalent implementation but faster\n",
    "            h, w = x.shape[2:]\n",
    "            if self.kernel_size[0]>=h and self.kernel_size[1]>=w:\n",
    "                out = F.adaptive_avg_pool2d(x,1)\n",
    "            else:\n",
    "                r1 = [r for r in self.rs if h%r==0][0]\n",
    "                r2 = [r for r in self.rs if w%r==0][0]\n",
    "                # reduction_constraint\n",
    "                r1 = min(self.max_r1, r1)\n",
    "                r2 = min(self.max_r2, r2)\n",
    "                s = x[:,:,::r1, ::r2].cumsum(dim=-1).cumsum(dim=-2)\n",
    "                n, c, h, w = s.shape\n",
    "                k1, k2 = min(h-1, self.kernel_size[0]//r1), min(w-1, self.kernel_size[1]//r2)\n",
    "                out = (s[:,:,:-k1,:-k2]-s[:,:,:-k1,k2:]-s[:,:,k1:,:-k2]+s[:,:,k1:,k2:])/(k1*k2)\n",
    "                out = torch.nn.functional.interpolate(out, scale_factor=(r1,r2))\n",
    "        else:\n",
    "            n, c, h, w = x.shape\n",
    "            s = x.cumsum(dim=-1).cumsum_(dim=-2)\n",
    "            s = torch.nn.functional.pad(s, (1,0,1,0)) # pad 0 for convenience\n",
    "            k1, k2 = min(h, self.kernel_size[0]), min(w, self.kernel_size[1])\n",
    "            s1, s2, s3, s4 = s[:,:,:-k1,:-k2],s[:,:,:-k1,k2:], s[:,:,k1:,:-k2], s[:,:,k1:,k2:]\n",
    "            out = s4+s1-s2-s3\n",
    "            out = out / (k1*k2)\n",
    "    \n",
    "        if self.auto_pad:\n",
    "            n, c, h, w = x.shape\n",
    "            _h, _w = out.shape[2:]\n",
    "            # print(x.shape, self.kernel_size)\n",
    "            pad2d = ((w - _w)//2, (w - _w + 1)//2, (h - _h) // 2, (h - _h + 1) // 2)\n",
    "            out = torch.nn.functional.pad(out, pad2d, mode='replicate')\n",
    "        \n",
    "        return "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.2 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1be8e1e697be24aa51be46f7515f4d96c6005120fc689094ce96895b044c9b1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
