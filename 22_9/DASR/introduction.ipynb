{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reference\n",
    "+ [official code](https://github.com/csjliang/DASR)\n",
    "+ [讲解MoE](https://zhuanlan.zhihu.com/p/542465517)\n",
    "+ [Dynamic convolution 代码详解](https://zhuanlan.zhihu.com/p/208519425)\n",
    "+ [Dynamic convolution 论文详解](https://zhuanlan.zhihu.com/p/142381725)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Related work\n",
    "+ 动态卷积\n",
    "+ MoE\n",
    "+ 对抗损失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# question\n",
    "## 什么是MoE(mixture of experts)？\n",
    "### 核心思想\n",
    "提出一种新的监督学习过程，一个系统中包含多个分开的网络，每个网络去处理全部训练样本的一个子集。假如我们已经知道数据集中存在一些天然的子集(比如：不同的模糊场景)。那么使用单个模型去学习就会收到很多干扰，导致学习缓慢、泛化困难。这时，我们可以使用**多个模型(expert)去学习**。使用一个**门网络来决定每个数据应该被哪一个模型去训练**，这样可以减少不同类型样本之间的干扰。\n",
    "## 什么是动态卷积(Dynamic convolution)？\n",
    "### 核心思想\n",
    "特点：动态卷积在不增加网络深度和宽度的前提下，增加网络的复杂度。\n",
    "\n",
    "1、动态卷积采用基于输入的注意力机制，动态地聚合多个并行的卷积核。\n",
    "\n",
    "2、将多个卷积核集合在一起，一方面，核的尺寸较小，具有较高的计算效率，另一方面，由于这些核采用非线性的方式聚集在一起，因此具有更强的表示能力。\n",
    "### 结构图\n",
    "<div align=center>\n",
    "<img src=.\\img\\介绍图.jpg>\n",
    "</div>\n",
    "\n",
    "训练时：所有参数都会进行更改\n",
    "\n",
    "推理时：红色框的参数是固定的，黄色框的参数随着输入数据的变化而更改\n",
    "\n",
    "## 模型退化参数如何获取？\n",
    "作者自定义的，详见代码DASR_model 75~420\n",
    "\n",
    "## DASR是如何使用动态卷积？\n",
    "采用动态卷积搭建MoE，详见代码arch_util 196~226"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型流程\n",
    "<div align=center>\n",
    "<img src=.\\img\\model.png>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Idea\n",
    "作者提出一种高效且有用的degradation-adaptive super-resolution(DASR)，该网络大致可以分为两部分：一个微小的回归网络被用来预测输入图像的退化参数，几个具有相同拓扑的convolutional experts联合优化，通过experts的非线性混合来指定网络参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Degradation_prediction_network(net_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Degradation_Predictor(nn.Module):\n",
    "    def __init__(self, in_nc=3, nf=64, num_params=100, num_networks=5, use_bias=True):\n",
    "        super(Degradation_Predictor, self).__init__()\n",
    "\n",
    "        self.ConvNet = nn.Sequential(*[\n",
    "            nn.Conv2d(in_nc, nf, kernel_size=5, stride=1, padding=2),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(nf, nf, kernel_size=5, stride=1, padding=2, bias=use_bias),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(nf, nf, kernel_size=5, stride=1, padding=2, bias=use_bias),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(nf, nf, kernel_size=5, stride=2, padding=2, bias=use_bias),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(nf, nf, kernel_size=5, stride=1, padding=2, bias=use_bias),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(nf, num_params, kernel_size=5, stride=1, padding=2, bias=use_bias),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "        ])\n",
    "\n",
    "        self.globalPooling = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        # model-A\n",
    "        self.MappingNet = nn.Sequential(*[\n",
    "            nn.Linear(num_params, 15),\n",
    "            nn.Linear(15, num_networks),\n",
    "        ])\n",
    "\n",
    "    def forward(self, input):\n",
    "        conv = self.ConvNet(input)\n",
    "        flat = self.globalPooling(conv)\n",
    "        out_params = flat.view(flat.size()[:2])\n",
    "        mapped_weights = self.MappingNet(out_params)\n",
    "        return out_params, mapped_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 超分(net_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSRResNetDynamic(nn.Module):\n",
    "\n",
    "    def __init__(self, num_in_ch=3, num_out_ch=3, num_feat=64, num_block=16, num_models=5, upscale=4):\n",
    "        super(MSRResNetDynamic, self).__init__()\n",
    "        self.upscale = upscale\n",
    "\n",
    "        self.conv_first = Dynamic_conv2d(num_in_ch, num_feat, 3, groups=1, if_bias=True, K=num_models)\n",
    "        self.body = make_layer(ResidualBlockNoBNDynamic, num_block, num_feat=num_feat, num_models=num_models)\n",
    "\n",
    "        # upsampling\n",
    "        if self.upscale in [2, 3]:\n",
    "            self.upconv1 = Dynamic_conv2d(num_feat, num_feat * self.upscale * self.upscale, 3, groups=1, if_bias=True, K=num_models)\n",
    "            self.pixel_shuffle = nn.PixelShuffle(self.upscale)\n",
    "        elif self.upscale == 4:\n",
    "            self.upconv1 = Dynamic_conv2d(num_feat, num_feat * 4, 3, groups=1, if_bias=True, K=num_models)\n",
    "            self.upconv2 = Dynamic_conv2d(num_feat, num_feat * 4, 3, groups=1, if_bias=True, K=num_models)\n",
    "            self.pixel_shuffle = nn.PixelShuffle(2)\n",
    "\n",
    "        self.conv_hr = Dynamic_conv2d(num_feat, num_feat, 3, groups=1, if_bias=True, K=num_models)\n",
    "        self.conv_last = Dynamic_conv2d(num_feat, num_out_ch, 3, groups=1, if_bias=True, K=num_models)\n",
    "\n",
    "        # activation function\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "\n",
    "\n",
    "    def forward(self, x, weights):\n",
    "        out = self.lrelu(self.conv_first({'x': x, 'weights': weights}))\n",
    "        out = self.body({'x': out, 'weights': weights})['x']\n",
    "\n",
    "        if self.upscale == 4:\n",
    "            out = self.lrelu(self.pixel_shuffle(self.upconv1({'x': out, 'weights': weights})))\n",
    "            out = self.lrelu(self.pixel_shuffle(self.upconv2({'x': out, 'weights': weights})))\n",
    "        elif self.upscale in [2, 3]:\n",
    "            out = self.lrelu(self.pixel_shuffle(self.upconv1({'x': out, 'weights': weights})))\n",
    "\n",
    "        out = self.lrelu(self.conv_hr({'x': out, 'weights': weights}))\n",
    "        out = self.conv_last({'x': out, 'weights': weights})\n",
    "        base = F.interpolate(x, scale_factor=self.upscale, mode='bilinear', align_corners=False)\n",
    "        out += base\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResidualBlockNoBNDynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlockNoBNDynamic(nn.Module):\n",
    "    \"\"\"Residual block without BN.\n",
    "\n",
    "    It has a style of:\n",
    "        ---Conv-ReLU-Conv-+-\n",
    "         |________________|\n",
    "\n",
    "    Args:\n",
    "        num_feat (int): Channel number of intermediate features.\n",
    "            Default: 64.\n",
    "        res_scale (float): Residual scale. Default: 1.\n",
    "        pytorch_init (bool): If set to True, use pytorch default init,\n",
    "            otherwise, use default_init_weights. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_feat=64, res_scale=1, num_models=5):\n",
    "        super(ResidualBlockNoBNDynamic, self).__init__()\n",
    "        self.res_scale = res_scale\n",
    "        self.conv1 = Dynamic_conv2d(num_feat, num_feat, 3, groups=1, if_bias=True, K=num_models)\n",
    "        self.conv2 = Dynamic_conv2d(num_feat, num_feat, 3, groups=1, if_bias=True, K=num_models)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        default_init_weights([self.conv1, self.conv2], 0.1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        identity = inputs['x'].clone()\n",
    "        out = self.relu(self.conv1(inputs))\n",
    "        conv2_input = {'x':out, 'weights':inputs['weights']}\n",
    "        out = self.conv2(conv2_input)\n",
    "        out = identity + out * self.res_scale\n",
    "        return {'x':out, 'weights':inputs['weights']}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.2 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1be8e1e697be24aa51be46f7515f4d96c6005120fc689094ce96895b044c9b1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
