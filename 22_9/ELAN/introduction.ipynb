{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reference\n",
    "+ [official code](https://github.com/xindongzhang/ELAN)\n",
    "+ [图像预处理之meanshift](https://blog.csdn.net/zerone_zjp/article/details/108704995)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  值得注意的地方\n",
    "+ 为什么用batchnormal而不是layernormal？\n",
    "  + EDSR介绍为什么用LayerNormal，而RepSR又引入了BatchNormal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# question\n",
    "## 作者在摘要部分为什么说shift-conv和1X1卷积具有相同等级的复杂度？\n",
    "因为shift-conv本质上就是1X1卷积，通过shift操作扩大感受野\n",
    "## 怎样理解shift-conv？\n",
    "<div align=center>\n",
    "<img src=.\\img\\shift-conv.jpeg>\n",
    "</div>\n",
    "\n",
    "## 怎样理解share-attention？\n",
    "<div align=center>\n",
    "<img src=.\\img\\share-attention.jpeg>\n",
    "</div>\n",
    "具体细节可以在ELAB中找到答案。可以认为是对于$\\theta_{i}$公用\n",
    "\n",
    "## Meanshift有什么作用？\n",
    "将0~255映射到更小的范围内，相当于数值缩小，之后再进行还原。Q:为什么很少在去模糊领域中遇见？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeanShift\n",
    "减去RGB均值。\n",
    "\n",
    "为什么每张图片都要减去数据集均值呢？\n",
    "\n",
    "原因：为了进行数据特征标准化，即像机器学习中的特征预处理那样对输入特征向量各维去均值再除以标准差，但由于自然图像各点像素值的范围都在0-255之间，方差大致一样，只要做去均值（减去整个图像数据集的均值或各通道关于图像数据集的均值）处理即可。\n",
    "\n",
    "主要原理：我们默认自然图像是一类平稳的数据分布(即数据每一维的统计都服从相同分布)，此时，在每个样本上减去数据的统计平均值可以移除共同的部分，凸显个体差异。\n",
    "\n",
    "在RCAN有所引用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# rgb_mean:换换值？\n",
    "class MeanShift(nn.Conv2d):\n",
    "    def __init__(\n",
    "        self, rgb_range,\n",
    "        rgb_mean=(0.4488, 0.4371, 0.4040), rgb_std=(1.0, 1.0, 1.0), sign=-1):\n",
    "        super(MeanShift, self).__init__(3, 3, kernel_size=1)\n",
    "        std = torch.Tensor(rgb_std)\n",
    "        self.weight.data = torch.eye(3).view(3, 3, 1, 1) / std.view(3, 1, 1, 1)\n",
    "        self.bias.data = sign * rgb_range * torch.Tensor(rgb_mean) / std\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELAB\n",
    "<div align=center>\n",
    "<img src=.\\img\\ELAB.jpeg>\n",
    "</div>\n",
    "\n",
    "#### ShiftConv2d0和ShiftConv2d1区别？\n",
    "+ ShiftConv2d0以低内存消耗(没有存储中间变量)的形式实现shiftConv；ShiftConv2d1以高速度(虽然采用两次卷积但是仅仅只有1X1Conv求梯度)的特点实现shiftConv\n",
    "### GMSA\n",
    "<div align=center>\n",
    "<img src=.\\img\\GMSA.jpeg>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "import numbers\n",
    "from torch.nn.utils import weight_norm\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "\n",
    "class ShiftConv2d0(nn.Module):\n",
    "    def __init__(self, inp_channels, out_channels):\n",
    "        super(ShiftConv2d0, self).__init__()    \n",
    "        self.inp_channels = inp_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.n_div = 5\n",
    "        g = inp_channels // self.n_div\n",
    "\n",
    "        conv3x3 = nn.Conv2d(inp_channels, out_channels, 3, 1, 1)\n",
    "        mask = nn.Parameter(torch.zeros((self.out_channels, self.inp_channels, 3, 3)), requires_grad=False)\n",
    "        mask[:, 0*g:1*g, 1, 2] = 1.0\n",
    "        mask[:, 1*g:2*g, 1, 0] = 1.0\n",
    "        mask[:, 2*g:3*g, 2, 1] = 1.0\n",
    "        mask[:, 3*g:4*g, 0, 1] = 1.0\n",
    "        mask[:, 4*g:, 1, 1] = 1.0\n",
    "        self.w = conv3x3.weight\n",
    "        self.b = conv3x3.bias\n",
    "        self.m = mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.conv2d(input=x, weight=self.w * self.m, bias=self.b, stride=1, padding=1) \n",
    "        return y\n",
    "\n",
    "\n",
    "class ShiftConv2d1(nn.Module):\n",
    "    def __init__(self, inp_channels, out_channels):\n",
    "        super(ShiftConv2d1, self).__init__()    \n",
    "        self.inp_channels = inp_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        ##！！！kernel weight\n",
    "        #### 注意看pytorch文档\n",
    "        # weight-filter of shape(out_channel,in_channels/groups,kernel_H,kernel_W)\n",
    "        self.weight = nn.Parameter(torch.zeros(inp_channels, 1, 3, 3), requires_grad=False)\n",
    "        self.n_div = 5\n",
    "        g = inp_channels // self.n_div\n",
    "        self.weight[0*g:1*g, 0, 1, 2] = 1.0 ## left\n",
    "        self.weight[1*g:2*g, 0, 1, 0] = 1.0 ## right\n",
    "        self.weight[2*g:3*g, 0, 2, 1] = 1.0 ## up\n",
    "        self.weight[3*g:4*g, 0, 0, 1] = 1.0 ## down\n",
    "        self.weight[4*g:, 0, 1, 1] = 1.0 ## identity     \n",
    "\n",
    "        self.conv1x1 = nn.Conv2d(inp_channels, out_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.conv2d(input=x, weight=self.weight, bias=None, stride=1, padding=1, groups=self.inp_channels)\n",
    "        y = self.conv1x1(y) \n",
    "        return y\n",
    "\n",
    "\n",
    "class ShiftConv2d(nn.Module):\n",
    "    def __init__(self, inp_channels, out_channels, conv_type='fast-training-speed'):\n",
    "        super(ShiftConv2d, self).__init__()    \n",
    "        self.inp_channels = inp_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.conv_type = conv_type\n",
    "        if conv_type == 'low-training-memory': \n",
    "            self.shift_conv = ShiftConv2d0(inp_channels, out_channels)\n",
    "        elif conv_type == 'fast-training-speed':\n",
    "            self.shift_conv = ShiftConv2d1(inp_channels, out_channels)\n",
    "        else:\n",
    "            raise ValueError('invalid type of shift-conv2d')\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.shift_conv(x)\n",
    "        return y\n",
    "# local feature extractio\n",
    "class LFE(nn.Module):\n",
    "    def __init__(self, inp_channels, out_channels, exp_ratio=4, act_type='relu'):\n",
    "        super(LFE, self).__init__()    \n",
    "        self.exp_ratio = exp_ratio\n",
    "        self.act_type  = act_type\n",
    "\n",
    "        self.conv0 = ShiftConv2d(inp_channels, out_channels*exp_ratio)\n",
    "        self.conv1 = ShiftConv2d(out_channels*exp_ratio, out_channels)\n",
    "\n",
    "        if self.act_type == 'linear':\n",
    "            self.act = None\n",
    "        elif self.act_type == 'relu':\n",
    "            self.act = nn.ReLU(inplace=True)\n",
    "        elif self.act_type == 'gelu':\n",
    "            self.act = nn.GELU()\n",
    "        else:\n",
    "            raise ValueError('unsupport type of activation')\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv0(x)\n",
    "        y = self.act(y)\n",
    "        y = self.conv1(y) \n",
    "        return y\n",
    "\n",
    "class GMSA(nn.Module):\n",
    "    def __init__(self, channels, shifts=4, window_sizes=[4, 8, 12], calc_attn=True):\n",
    "        super(GMSA, self).__init__()    \n",
    "        self.channels = channels\n",
    "        self.shifts   = shifts\n",
    "        self.window_sizes = window_sizes\n",
    "        self.calc_attn = calc_attn\n",
    "\n",
    "        if self.calc_attn:\n",
    "            self.split_chns  = [channels*2//3, channels*2//3, channels*2//3]\n",
    "            self.project_inp = nn.Sequential(\n",
    "                nn.Conv2d(self.channels, self.channels*2, kernel_size=1), \n",
    "                nn.BatchNorm2d(self.channels*2)\n",
    "            )\n",
    "            self.project_out = nn.Conv2d(channels, channels, kernel_size=1)\n",
    "        else:\n",
    "            self.split_chns  = [channels//3, channels//3,channels//3]\n",
    "            self.project_inp = nn.Sequential(\n",
    "                nn.Conv2d(self.channels, self.channels, kernel_size=1), \n",
    "                nn.BatchNorm2d(self.channels)\n",
    "            )\n",
    "            self.project_out = nn.Conv2d(channels, channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, prev_atns = None):\n",
    "        b,c,h,w = x.shape\n",
    "        x = self.project_inp(x)\n",
    "        xs = torch.split(x, self.split_chns, dim=1)\n",
    "        ys = []\n",
    "        atns = []\n",
    "        if prev_atns is None:\n",
    "            for idx, x_ in enumerate(xs):\n",
    "                wsize = self.window_sizes[idx]\n",
    "                if self.shifts > 0:\n",
    "                    x_ = torch.roll(x_, shifts=(-wsize//2, -wsize//2), dims=(2,3))\n",
    "                q, v = rearrange(\n",
    "                    x_, 'b (qv c) (h dh) (w dw) -> qv (b h w) (dh dw) c', \n",
    "                    qv=2, dh=wsize, dw=wsize\n",
    "                )\n",
    "                atn = (q @ q.transpose(-2, -1)) \n",
    "                atn = atn.softmax(dim=-1)\n",
    "                y_ = (atn @ v)\n",
    "                y_ = rearrange(\n",
    "                    y_, '(b h w) (dh dw) c-> b (c) (h dh) (w dw)', \n",
    "                    h=h//wsize, w=w//wsize, dh=wsize, dw=wsize\n",
    "                )\n",
    "                if self.shifts > 0:\n",
    "                    y_ = torch.roll(y_, shifts=(wsize//2, wsize//2), dims=(2, 3))\n",
    "                ys.append(y_)\n",
    "                atns.append(atn)\n",
    "            y = torch.cat(ys, dim=1)            \n",
    "            y = self.project_out(y)\n",
    "            return y, atns\n",
    "        else:\n",
    "            for idx, x_ in enumerate(xs):\n",
    "                wsize = self.window_sizes[idx]\n",
    "                if self.shifts > 0:\n",
    "                    x_ = torch.roll(x_, shifts=(-wsize//2, -wsize//2), dims=(2,3))\n",
    "                atn = prev_atns[idx]\n",
    "                v = rearrange(\n",
    "                    x_, 'b (c) (h dh) (w dw) -> (b h w) (dh dw) c', \n",
    "                    dh=wsize, dw=wsize\n",
    "                )\n",
    "                y_ = (atn @ v)\n",
    "                y_ = rearrange(\n",
    "                    y_, '(b h w) (dh dw) c-> b (c) (h dh) (w dw)', \n",
    "                    h=h//wsize, w=w//wsize, dh=wsize, dw=wsize\n",
    "                )\n",
    "                if self.shifts > 0:\n",
    "                    y_ = torch.roll(y_, shifts=(wsize//2, wsize//2), dims=(2, 3))\n",
    "                ys.append(y_)\n",
    "            y = torch.cat(ys, dim=1)            \n",
    "            y = self.project_out(y)\n",
    "            return y, prev_atns\n",
    "\n",
    "class ELAB(nn.Module):\n",
    "    def __init__(self, inp_channels, out_channels, exp_ratio=2, shifts=0, window_sizes=[4, 8, 12], shared_depth=1):\n",
    "        super(ELAB, self).__init__()\n",
    "        self.exp_ratio = exp_ratio\n",
    "        self.shifts = shifts\n",
    "        self.window_sizes = window_sizes\n",
    "        self.inp_channels = inp_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.shared_depth = shared_depth\n",
    "        \n",
    "        modules_lfe = {}\n",
    "        modules_gmsa = {}\n",
    "        modules_lfe['lfe_0'] = LFE(inp_channels=inp_channels, out_channels=out_channels, exp_ratio=exp_ratio)\n",
    "        modules_gmsa['gmsa_0'] = GMSA(channels=inp_channels, shifts=shifts, window_sizes=window_sizes, calc_attn=True)\n",
    "        for i in range(shared_depth):\n",
    "            modules_lfe['lfe_{}'.format(i+1)] = LFE(inp_channels=inp_channels, out_channels=out_channels, exp_ratio=exp_ratio)\n",
    "            modules_gmsa['gmsa_{}'.format(i+1)] = GMSA(channels=inp_channels, shifts=shifts, window_sizes=window_sizes, calc_attn=False)\n",
    "        self.modules_lfe = nn.ModuleDict(modules_lfe)\n",
    "        self.modules_gmsa = nn.ModuleDict(modules_gmsa)\n",
    "\n",
    "    def forward(self, x):\n",
    "        atn = None\n",
    "        for i in range(1 + self.shared_depth):\n",
    "            if i == 0: ## only calculate attention for the 1-st module\n",
    "                x = self.modules_lfe['lfe_{}'.format(i)](x) + x\n",
    "                y, atn = self.modules_gmsa['gmsa_{}'.format(i)](x, None)\n",
    "                x = y + x\n",
    "            else:\n",
    "                x = self.modules_lfe['lfe_{}'.format(i)](x) + x\n",
    "                y, atn = self.modules_gmsa['gmsa_{}'.format(i)](x, atn)\n",
    "                x = y + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## network\n",
    "<div align=center>\n",
    "<img src=.\\img\\network.jpeg>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "import numbers\n",
    "from torch.nn.utils import weight_norm\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from models.elan_block import ELAB, MeanShift\n",
    "\n",
    "def create_model(args):\n",
    "    return ELAN(args)\n",
    "\n",
    "class ELAN(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(ELAN, self).__init__()\n",
    "\n",
    "        self.scale = args.scale\n",
    "        self.colors = args.colors\n",
    "        self.window_sizes = args.window_sizes\n",
    "        self.m_elan  = args.m_elan\n",
    "        self.c_elan  = args.c_elan\n",
    "        self.n_share = args.n_share\n",
    "        self.r_expand = args.r_expand\n",
    "        self.sub_mean = MeanShift(args.rgb_range)\n",
    "        self.add_mean = MeanShift(args.rgb_range, sign=1)\n",
    "\n",
    "        # define head module\n",
    "        m_head = [nn.Conv2d(self.colors, self.c_elan, kernel_size=3, stride=1, padding=1)]\n",
    "\n",
    "        # define body module\n",
    "        m_body = []\n",
    "        for i in range(self.m_elan // (1+self.n_share)):\n",
    "            if (i+1) % 2 == 1: \n",
    "                m_body.append(\n",
    "                    ELAB(\n",
    "                        self.c_elan, self.c_elan, self.r_expand, 0, \n",
    "                        self.window_sizes, shared_depth=self.n_share\n",
    "                    )\n",
    "                )\n",
    "            else:              \n",
    "                m_body.append(\n",
    "                    ELAB(\n",
    "                        self.c_elan, self.c_elan, self.r_expand, 1, \n",
    "                        self.window_sizes, shared_depth=self.n_share\n",
    "                    )\n",
    "                )\n",
    "        # define tail module\n",
    "        m_tail = [\n",
    "            nn.Conv2d(self.c_elan, self.colors*self.scale*self.scale, kernel_size=3, stride=1, padding=1),\n",
    "            nn.PixelShuffle(self.scale)\n",
    "        ]\n",
    "\n",
    "        self.head = nn.Sequential(*m_head)\n",
    "        self.body = nn.Sequential(*m_body)\n",
    "        self.tail = nn.Sequential(*m_tail)\n",
    "\n",
    "    def forward(self, x):\n",
    "        H, W = x.shape[2:]\n",
    "        x = self.check_image_size(x)\n",
    "        \n",
    "        x = self.sub_mean(x)\n",
    "        x = self.head(x)\n",
    "        res = self.body(x)\n",
    "        res = res + x\n",
    "        x = self.tail(res)\n",
    "        x = self.add_mean(x)\n",
    "\n",
    "        return x[:, :, 0:H*self.scale, 0:W*self.scale]\n",
    "\n",
    "    def check_image_size(self, x):\n",
    "        _, _, h, w = x.size()\n",
    "        wsize = self.window_sizes[0]\n",
    "        for i in range(1, len(self.window_sizes)):\n",
    "            wsize = wsize*self.window_sizes[i] // math.gcd(wsize, self.window_sizes[i])\n",
    "        mod_pad_h = (wsize - h % wsize) % wsize\n",
    "        mod_pad_w = (wsize - w % wsize) % wsize\n",
    "        x = F.pad(x, (0, mod_pad_w, 0, mod_pad_h), 'reflect')\n",
    "        return x\n",
    "\n",
    "    def load_state_dict(self, state_dict, strict=True):\n",
    "        own_state = self.state_dict()\n",
    "        for name, param in state_dict.items():\n",
    "            if name in own_state:\n",
    "                if isinstance(param, nn.Parameter):\n",
    "                    param = param.data\n",
    "                try:\n",
    "                    own_state[name].copy_(param)\n",
    "                except Exception:\n",
    "                    if name.find('tail') == -1:\n",
    "                        raise RuntimeError('While copying the parameter named {}, '\n",
    "                                           'whose dimensions in the model are {} and '\n",
    "                                           'whose dimensions in the checkpoint are {}.'\n",
    "                                           .format(name, own_state[name].size(), param.size()))\n",
    "            elif strict:\n",
    "                if name.find('tail') == -1:\n",
    "                    raise KeyError('unexpected key \"{}\" in state_dict'\n",
    "                                   .format(name))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9116fc67e4256ae2032cd97a5d43eaf97776cee9d03cd8e1c3e900c0daf90596"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
