{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# question?\n",
    "+ 大的卷积核如何训练？\n",
    "  后来放弃了\n",
    "+ 大的卷积核忽视的局部信息如何弥补？\n",
    "  Fused-MBConv\n",
    "+ channel projection里的channel shuffling 如何实现？\n",
    "```\n",
    "rearrange(x, 'b (g d) h w -> b (d g) h w', g=8)\n",
    "```\n",
    "+ 为什么Fused-MBConvs可以弥补？\n",
    "  3X3卷积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numbers\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "from einops import rearrange\n",
    "from basicsr.utils.registry import ARCH_REGISTRY\n",
    "\n",
    "\n",
    "\n",
    "def to_3d(x):\n",
    "    return rearrange(x, 'b c h w -> b (h w) c')\n",
    "\n",
    "def to_4d(x, h, w):\n",
    "    return rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "\n",
    "\n",
    "class BiasFree_LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape):\n",
    "        super(BiasFree_LayerNorm, self).__init__()\n",
    "        if isinstance(normalized_shape, numbers.Integral):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        normalized_shape = torch.Size(normalized_shape)\n",
    "\n",
    "        assert len(normalized_shape) == 1\n",
    "\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.normalized_shape = normalized_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu = x.mean(-1, keepdim=True)\n",
    "        sigma = x.var(-1, keepdim=True, unbiased=False)\n",
    "        return (x - mu) / torch.sqrt(sigma + 1e-5) * self.weight\n",
    "        # return x / torch.sqrt(sigma+1e-5) * self.weight\n",
    "\n",
    "\n",
    "class WithBias_LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape):\n",
    "        super(WithBias_LayerNorm, self).__init__()\n",
    "        if isinstance(normalized_shape, numbers.Integral):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        normalized_shape = torch.Size(normalized_shape)\n",
    "\n",
    "        assert len(normalized_shape) == 1\n",
    "\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.normalized_shape = normalized_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu = x.mean(-1, keepdim=True)\n",
    "        sigma = x.var(-1, keepdim=True, unbiased=False)\n",
    "        return (x - mu) / torch.sqrt(sigma + 1e-5) * self.weight + self.bias\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, LayerNorm_type='BiasFree'):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        if LayerNorm_type == 'BiasFree':\n",
    "            self.body = BiasFree_LayerNorm(dim)\n",
    "        else:\n",
    "            self.body = WithBias_LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[-2:]\n",
    "        return to_4d(self.body(to_3d(x)), h, w)\n",
    "\n",
    "class PointMlp(nn.Module):\n",
    "    def __init__(self, dim, mlp_ratio=4):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(dim * mlp_ratio)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(dim, hidden_dim, 1, 1, 0),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Conv2d(hidden_dim, dim, 1, 1, 0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "##?????????????????????\n",
    "class SplitPointMlp(nn.Module):\n",
    "    def __init__(self, dim, mlp_ratio=2):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(dim//2 * mlp_ratio)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(dim//2, hidden_dim, 1, 1, 0),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Conv2d(hidden_dim, dim//2, 1, 1, 0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, x2 = x.chunk(2, dim=1)\n",
    "        x1 = self.fc(x1)\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        return rearrange(x, 'b (g d) h w -> b (d g) h w', g=8)\n",
    "\n",
    "\n",
    "# Shuffle Mixing layer\n",
    "class SMLayer(nn.Module):\n",
    "    def __init__(self, dim, kernel_size, mlp_ratio=2):\n",
    "        super().__init__()\n",
    "        self.norm1 = LayerNorm(dim)\n",
    "        self.norm2 = LayerNorm(dim)\n",
    "\n",
    "        self.spatial = nn.Conv2d(dim, dim, kernel_size, 1, kernel_size // 2, groups=dim)\n",
    "\n",
    "        self.mlp1 = SplitPointMlp(dim, mlp_ratio)\n",
    "        self.mlp2 = SplitPointMlp(dim, mlp_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mlp1(self.norm1(x)) + x\n",
    "        x = self.spatial(x)\n",
    "        x = self.mlp2(self.norm2(x)) + x\n",
    "        return x\n",
    "\n",
    "\n",
    "# Feature Mixing Block\n",
    "class FMBlock(nn.Module):\n",
    "    def __init__(self, dim, kernel_size, mlp_ratio=2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            SMLayer(dim, kernel_size, mlp_ratio),\n",
    "            SMLayer(dim, kernel_size, mlp_ratio),\n",
    "        )\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(dim, dim + 16, 3, 1, 1),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Conv2d(dim + 16, dim, 1, 1, 0)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x) + x\n",
    "        x = self.conv(x) + x \n",
    "        return x\n",
    "\n",
    "\n",
    "@ARCH_REGISTRY.register()\n",
    "class ShuffleMixer(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        n_feats (int): Number of channels. Default: 64 (32 for the tiny model).\n",
    "        kerenl_size (int): kernel size of Depthwise convolution. Default:7 (3 for the tiny model).\n",
    "        n_blocks (int): Number of feature mixing blocks. Default: 5.\n",
    "        mlp_ratio (int): The expanding factor of point-wise MLP. Default: 2.\n",
    "        upscaling_factor: The upscaling factor. [2, 3, 4]\n",
    "    \"\"\"\n",
    "    def __init__(self, n_feats=64, kernel_size=7, n_blocks=5, mlp_ratio=2, upscaling_factor=4):\n",
    "        super().__init__()\n",
    "        self.scale = upscaling_factor\n",
    "\n",
    "        self.to_feat = nn.Conv2d(3, n_feats, 3, 1, 1, bias=False)\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[FMBlock(n_feats, kernel_size, mlp_ratio) for _ in range(n_blocks)]\n",
    "        )\n",
    "\n",
    "        if self.scale == 4:\n",
    "            self.upsapling = nn.Sequential(\n",
    "                nn.Conv2d(n_feats, n_feats * 4, 1, 1, 0),\n",
    "                nn.PixelShuffle(2),\n",
    "                nn.SiLU(inplace=True),\n",
    "                nn.Conv2d(n_feats, n_feats * 4 , 1, 1, 0),\n",
    "                nn.PixelShuffle(2),\n",
    "                nn.SiLU(inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            self.upsapling = nn.Sequential(\n",
    "                nn.Conv2d(n_feats, n_feats * self.scale * self.scale, 1, 1, 0),\n",
    "                nn.PixelShuffle(self.scale),\n",
    "                nn.SiLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        self.tail = nn.Conv2d(n_feats, 3, 3, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        base = x\n",
    "        x = self.to_feat(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.upsapling(x)\n",
    "        x = self.tail(x)\n",
    "        base = F.interpolate(base, scale_factor=self.scale, mode='bilinear', align_corners=False)\n",
    "        return x + base\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from fvcore.nn import flop_count_table, FlopCountAnalysis, ActivationCountAnalysis\n",
    "    # 720p [1280 * 720]\n",
    "    # x = torch.randn(1, 3, 640, 360)\n",
    "    # x = torch.randn(1, 3, 427, 240)\n",
    "    # x = torch.randn(1, 3, 320, 180)\n",
    "    x = torch.randn(1, 3, 256, 256)\n",
    "\n",
    "    model = ShuffleMixer(n_feats=32, kernel_size=21, n_blocks=5, mlp_ratio=2, upscaling_factor=4)\n",
    "    print(model)\n",
    "    print(flop_count_table(FlopCountAnalysis(model, x), activations=ActivationCountAnalysis(model, x)))\n",
    "    output = model(x)\n",
    "    print(f'output: {output.shape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "030c80840b0d9f5db79d93d54f37f04cee5e760b761780c141ee4f016f609d74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
