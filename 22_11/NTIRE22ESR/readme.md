# trick1
Modifying the architecture of information multidistillation block (IMDB) and residual feature distillation block (RFDB) is the mainstream technique.
# trick2
Multi-stage information distillation might influence the inference speed of the super efficient models.
# trick3
Reparameterization could bring slight performance improvement.
# trick4
Filter decomposition methods could effectively reduce the model complexity.
# trick5
Network pruning began to play a role
# trick6
Activation function is an important factor.
# trick7
Design of loss functions is also among the consideration.
# trick8
Advanced training strategy guarantees the performance of the network.
# trick9
Various other techniques are also attempted